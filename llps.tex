\chapter{Locally-linear pictorial structures}\label{sec:llps}
\section{Introduction}
\out{
Human pose estimation from single, 2D images holds great potential to assist in 
a wide range of applications---semantic indexing of images and videos, action 
recognition, activity analysis, and human computer interaction, to name a few.
However, human pose estimation ``in the wild'' is an extremely challenging 
problem.  It shares all of the difficulties of object detection, such as 
confounding background clutter, lighting, viewpoint, and scale, in addition to
significant difficulties unique to human poses.  }


In this chapter, we focus explicitly on the multi-modal aspect of the 2D pose 
estimation problem.  As discussed in~\secref{perceptual}, there is an enormous 
variation in foreground and background color and texture, and even given 
viewpoint and pose, the shape of body parts is highly variable due to clothing, 
relative scale variations, and articulation (causing foreshortening, 
self-occlusion and physically different body contours).

\begin{figure}[t!]
\centering
\includegraphics[width=0.99\linewidth]{figs/llps-overview.pdf}
\caption{\label{fig:overview} \textbf{Left:} Most pictorial structures 
researchers have put effort into better and larger feature spaces, in which 
they fit one linear model.  The feature computation is expensive, and we still 
fail at capturing the many appearance modes in real data.  \textbf{Right:} We 
take a different approach.  Rather than introduce more and more features in 
hopes of high-dimensional linear separability, we model the non-linearities in 
simpler, lower dimensional feature spaces, using a collection of {\em locally} 
linear models.}
\end{figure}


As a motivating example, consider the multitude of variations of left elbows, 
ignoring color and lighting variations.  First, consider the range of body 
types---infant elbows and adult elbows, male elbows and female elbows, fat 
elbows and skinny---a conservative estimate would say there are 5 coarse body 
type modes.  Considering clothing, there are baggy long sleeves, tight long 
sleeves, stripes, solids, bare arms, and short sleeves ending near the 
elbow---let's assume we can summarize clothing variation around elbows with 5 
appearance modes (also overly optimistic).  Finally, given body type and 
clothing, we still must consider articulation.  A typical discretization of 
angles to model human pose~\cite{felz05,devacrf,eichner09} uses $15^\circ$ 
increments for a total of 24 possible relative angles.  As a rough 
back-of-the-envelope calculation, this already amounts to $5 \times 5 \times 24 
= 600$ major appearance modes, just for the elbow.  See Figure~\ref{fig:lelbs} 
for an illustration.


Clearly, some degree of sharing between appearance modes is warranted and 
beneficial, in particular because it is easier to estimate fewer model 
parameters.  However, most models developed for this problem take sharing to an 
extreme, and use {\em just a single, linear 
model}~\cite{devacrf,eichner09,sapp2010cascades,sapp2011,andriluka09,ddtran}.  
Such models make it very difficult to capture the part variations discussed.  
Most researchers focus attention on improving features in hopes that a better 
feature space will make a linear model work.  Only recently, several papers 
studied increasing the number of modes in pose models. However, these works 
used at most only tens of modes per part, and struggle with discovery of latent 
modes and/or defining an effective discrete quantization of the pose space.  
See Section~\ref{sec:rel} for details.

\begin{figure}[tb!]
\centering
\includegraphics[width=0.99\linewidth]{figs/lelbs.jpg}
\caption{ \label{fig:lelbs} A random sampling of 100 left elbows from the Buffy 
Stickmen pose dataset, removing color and intensity bias, to illustrate the 
huge variety of appearance due to clutter, motion blur, clothing, body type, 
and pose.}
\end{figure}

\mypar{Our approach:} In this chapter, we propose a model of human pose that 
defines a mode {\em per training example}, to capture a local neighborhood of
pose {\em and } appearance.  This results in hundreds or thousands of modes in 
contrast to the 20 or fewer used by recent work. This gives us the ability to 
precisely model pose and appearance using a linear, structured model for each 
local neighborhood.  Because of this defining characteristic, we call our model 
\LLPSlong~(\LLPS).

We define the extent of each neighborhood in terms of radii in appearance and 
pose, which decompose at a part level.  As a result, the training example space 
is well covered, and we have overlap between modes.  This allows us to heavily 
share training data and gives us a smoothly varying representation of the space 
of 2D human poses.

Using our explicit formulation of locality, no latent modes need to estimated, 
and no decisions made as to quantization degree and mode placement.  We propose 
a simple convex max-margin learning objective to fit parameters of our model.  
We show our model is competitive with state-of-the-art approaches from this 
year, while employing only very simple features. Further, inference is cheap 
and heavily parallelizable, taking only a few seconds per image.


\section{Model}\label{sec:model}

We pose the problem of 2D human pose estimation as a structured prediction task.  Let $x$ represent a given input image, and $y$ represent the location of $P$ parts in image coordinates.  Each variable $y_i$ denotes the pixel coordinates (row, column) of part $i$ in image $x$.  For ``parts'' we choose to model joints and their midpoints (\eg, left wrist, left forearm, left elbow) which allows us fine-grained encoding of foreshortening and rotation, as is done in~\cite{deva2011,sapp2011}.

The standard Pictorial Structures model~\cite{felzps} is a linear model which decomposes into a sum of unary and pairwise linear terms.  The score for a part configuration specified by $y$ in image $x$ is:
\begin{align}
 s(x,y) = \sum_{i=1}^P w_i \cdot f_i(x,y_i) + \sum_{(i,j) \in \mathcal{E}} w_{ij} \cdot f_{ij}(x,y_i,y_j),
 \label{eqn:ps}
 \end{align}
The set $\mathcal{E}$ denotes the edge set of pairs of connected variables in our model.  When this edge set describes a tree, we can use efficient dynamic programming techniques to infer the best scoring configuration of all parts:
\begin{align}
y^\star = \argmax_{y \in \mathcal{Y}} \;\;s(x,y).
\end{align}
The set $\mathcal{Y}$ denotes the entire set of possible poses, which is exponential in the number of model parts: $|\mathcal{Y}| = |\mathcal{Y}_i|^P$, where $\mathcal{Y}_i$ is the set of possible placements of part $i$ in the image (and is the same for all $i$).

\subsection{\LLPSlong~(\LLPS)}
Instead of a single, linear model as in Equation~\ref{eqn:ps}, we model human pose with a collection of linear models which describe different local neighborhoods.  Let us consider $M$ such models, which we index $m = 1 \ldots M$:
\begin{align}
s^m(x,y) = \sum_{i=1}^P w^m_i \cdot f_i(x,y_i) + \sum_{(i,j) \in \mathcal{E}} w^m_{ij} \cdot f_{ij}(x,y_i,y_j),
\end{align}
Each model shares the same tree structure $\mathcal{E}$ and features $f_i(\cdot),\; f_{ij}(\cdot)$ for the sake of simplicity; it is easy to extend this to a heterogenous collection of models.  The $M$ models' parameters $w^m = [w^m_i;~w^m_{ij}]$ are learned to fit a local neighborhood around each training exemplar, discussed in Section~\ref{sec:learning}.

The full LLPS model introduces an extra variable $y_m \in [1,M]$ into the state of the model to infer, at test time, both the best local model to use, and the best placement of joints given that model:
\begin{align}
s(x,y,y_m) &= s^m(x,y) \\
y^\star &= \argmax_{y_m,\; y \in \mathcal{Y}} s(x,y,y_m) 
\end{align}
Thus, given a test example, the test time inference procedure is straightforward (see Figure~\ref{fig:inference}): evaluate all $M$ local models (in practice, up to thousands of them), via max-product inference, saving the score and highest scoring output sequence of each.  Then, we take the highest scoring of the $M$ local models and its output sequence as a prediction of the pose.  This is transparently parallelizable and fits easily into a popular parallelization frameworks, such as MapReduce.

\begin{figure}[tb!]
\centering
\includegraphics[width=0.99\linewidth]{figs/empty.jpg}
\caption{\label{fig:inference} An illustration of the inference process.  Each 
training example defines a local neighborhood model.  Each model is run in 
parallel on a test image, and the argmax prediction is taken as a guess. Inset: 
the graphical model structure.}
\end{figure}

\section{Learning}\label{sec:learning}

During training, we have access to a training set of images with labeled poses $S = \{(x^{t},y^{t})\}_{t=1}^T$.  As described, we choose to model a local neighborhood in pose and appearance centered around each training example.  This allows us to capture fine variations of appearance from pose, clothing, lighting, etcetera using a single linear model.

We define a neighborhood set function which maps an image and corresponding human pose to a set of indices of ``close'' training samples: $t \in \mathcal{N}((x,y))$ means that training sample $t$ is somehow ``close'' to $(x,y)$ in appearance and pose.  We will also overload notation to say the $\mathcal{N}(t)$ refers to the neighborhood of training sample $t$.  The specifics of $\mathcal{N}(\cdot)$ which formalize the notions of closeness in pose and appearance are detailed in Section~\ref{sec:nbhd}.

The key idea of learning is that we want to fit parameters so that each local model scores higher than other models that are not part of its local neighborhood, on the correct pose configuration.  For submodel $m$ centered about training sample $(x^m,y^m)$, we would like the large-margin constraints:
\begin{align}
s^m(x^t,y^t) \geq 1 + s^{m}(x^t,y), \label{c1}\\
s^m(x^t,y^t) \geq 1 + s^{m'}(x^t,y),\label{c2}\\
\forall y \in \mathcal{Y}\setminus \{y^t\},~\forall t \in 
\mathcal{N}(m),~\forall m' \notin \mathcal{N}(m) \nonumber
\end{align}
In words, Equation~\ref{c1} states that the score of the true configuration for 
local model $m$ must be higher than $m$'s score for any other (wrong) 
configuration in example $t$.  Equation~\ref{c2} states that the score of the 
true configuration for $m$ must also be higher than any score a ``stranger'' 
model $m' \notin \mathcal{N}(m)$ has for any pose configuration in example $t$.  
Adding slack and regularization on the parameters, we get a convex, 
large-margin structured loss jointly over all $m$ local models

\begin{align}\label{eqn:full-learn}
\min_{\{w^m\}, \{\xi_{m,t}\}}& \;\; \sum_{m=1}^M ||w^m||_2^2 + C \sum_{m=1}^M 
\sum_{t \in \mathcal{N}(m)} \left[ \xi_{m,t} + \sum_{m' \notin \mathcal{N}(m)} 
\xi'_{m',t} \right] \\
\text{\bf{subject to:} }
&s^m(x^t,y^t) \geq 1 + s^{m}(x^t,y) - \xi_{m,t}\\
&s^m(x^t,y^t) \geq 1 + s^{m'}(x^t,y) - \xi'_{m',t}\\
&\forall y \in \mathcal{Y}\setminus \{y^t\}, \forall m \in [1,M], \forall t \in 
\mathcal{N}(m), \forall m' \notin \mathcal{N}(m) \nonumber \end{align}

Note that there is in practice significant overlap in neighborhood sets rather than disjoint partitions.  This gives a smoothly varying description of pose space.

\subsection{Decomposable approximate learning}
The learning objective in Equation~\ref{eqn:full-learn} couples all models together to train them jointly.  While this has attractive properties, it is unwieldy for large training sets.  We now propose a series of modifications to the form of the model and Equation~\ref{eqn:full-learn} to significantly simplify and parallelize training.

\mypar{A simpler pairwise cost:}
Traditionally, the pairwise cost in PS models has been a simple geometric displacement cost: $f_{ij}(x,y_i,y_j) = d(y_i - y_j - \delta_{ij})$, where $\delta_{ij}$ is the mean pixel displacement vector between parts $i$ and $j$, and $d(\cdot)$ is either a quadratic penalty~\cite{felzps} or a binning function~\cite{deva2006}.  These restricted forms of $d(\cdot)$ allow for fast inference message passing techniques via convolution or distance transforms.  We use instead a uniform {\em box deformation cost} defined by $f_{ij}(x,y_i,y_j) = 0$ when $|y_i - y_j - \delta_ij| < b$, and $-\infty$ otherwise, where $b$ specifies the size of the box in which, given $y_j$, $y_i$ is free to deviate from $\delta_{ij}$ in pixel units.  This yields a new form for our local model 
\begin{align}\label{eqn:boxdist}
s^m(x,y) = 
\begin{cases}
 \sum_i w^m_i \cdot f_i(x,y_i), & |y_i - y_j - \delta_{ij}| < b,\;\; \forall (i,j) \in \mathcal{E} \\
 -\infty & \text{otherwise}
 \end{cases}
\end{align}

This formulation has attractive properties: (1) It leads to even faster inference than a quadratic stretching cost, using a 2D min transform instead of a distance transform, (2) less parameters to learn in each local neighborhood.

\mypar{Decoupling local models:} Next, we drop the constraints
$$s^m(x^t,y^t) \geq 1 + s^{m'}(x^t,y), \forall m, \forall m'\notin \mathcal{N}(m),\forall t$$ 
which decouples model $m$'s parameters from all other models $m'$.  Model $m$ no longer has to score the correct configuration $y^t$ higher than other models' beliefs in the correct configuration.  Instead, it only has to score the correct configuration higher than the exponentially many incorrect configurations in its local neighborhood of training images.  This is equivalent to training a single PS model on a subset of the training data, independent of other models trained on other, overlapping subsets of the training data.  The inherent problem with this is that now different local model scores may not be comparable.  We address this with a calibration step described in Section~\ref{sec:calib}.

\mypar{Psuedolikelihood} A final approximation is to assume, for training, that the structured functions $s^m(x,y)$ decompose into independent cliques, and can thus be trained independently.  That is,
$$ s^m(x,y) = \sum_{i} s^m(x,y_i | y_{\mathcal{E}(i)}) $$ 
where $\mathcal{E}(i) = \{j \;|\; (i,j) \in \mathcal{E}\}$.  This is a common trick that greatly simplifies structured learning, at the cost of approximation error. 
Combined with the pairwise cost described in Equation~\ref{eqn:boxdist}, this amounts to
$$s^m(x,y_i| y_{\mathcal{E}(i)}) = 
\begin{cases}
 w^m_i \cdot f_i(x,y_i), & |y_i - y_j - \delta_{ij}| < b,\;\; \forall j \in \mathcal{E}(i) \\
 -\infty & \text{otherwise}
\end{cases}
$$
which means we can learn parameters $w_i^m$ separately for each part $i$ in 
each local model $m$.

In summary, by using a box deformation cost, decoupling the local models and using pseudolikelihood, we train part detectors individually.  The structured SVM learning objective for each part $i$ in each model $m$ is now:

\begin{align}
\min_{w_i^m}&~||w_i^m||_2^2 + C \sum_{t} \xi_t\\
\text{subject to: }
& w_i^m \cdot f_i(x^t,y_i^t) \geq 1+w_i^m \cdot f_i(x^t,y_i)-\xi_t \nonumber \\
& w_i^m \cdot f_i(x^t,y_i^t) \geq 1+w_i^m \cdot f_i(x^{t'},y_i)-\xi_t \nonumber 
\\
&\forall y_i \in \mathcal{Y}_i\setminus \{y_i^t\}, \forall t \in 
\mathcal{N}(m), \forall t' \notin \mathcal{N}(m) \nonumber
\end{align}
We can optimize this using an off-the-shelf standard binary SVM solver.  We 
handle the enormous number of constraints---$O(M \cdot |\mathcal{Y}_i|)$, the 
number of pixel locations in our dataset---by employing a cutting plane method, 
which starts from a random sampling of constraints, and iteratively finds 
violated constraints by mining for false positives, then re-optimizing.

	
